"""
Module pour la d√©finition et les utilitaires du mod√®le PyTorch.
Contient la classe TextClassifierNet et les fonctions d'affichage de progression.
"""

import torch
import torch.nn as nn

class TextClassifierNet(nn.Module):
    """
    R√©seau de neurones avanc√© pour la classification de texte - Version exacte du backup.
    Architecture sophistiqu√©e avec connexions r√©siduelles et diverses fonctions d'activation.
    """
    
    def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.4):
        """
        Initialise le r√©seau de neurones avanc√©.
        
        Args:
            input_size (int): Taille d'entr√©e (nombre de features TF-IDF)
            hidden_size (int): Taille de la couche cach√©e
            num_classes (int): Nombre de classes de sortie
            dropout_rate (float): Taux de dropout pour la r√©gularisation
        """
        super(TextClassifierNet, self).__init__()
        # Architecture plus profonde et sophistiqu√©e
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc4 = nn.Linear(hidden_size // 2, hidden_size // 4)
        self.fc5 = nn.Linear(hidden_size // 4, num_classes)
        
        # Normalisation et r√©gularisation
        self.dropout = nn.Dropout(dropout_rate)
        self.batch_norm1 = nn.BatchNorm1d(hidden_size)
        self.batch_norm2 = nn.BatchNorm1d(hidden_size)
        self.batch_norm3 = nn.BatchNorm1d(hidden_size // 2)
        self.batch_norm4 = nn.BatchNorm1d(hidden_size // 4)
        
        # Fonctions d'activation vari√©es
        self.relu = nn.ReLU()
        self.leaky_relu = nn.LeakyReLU(0.1)
        self.elu = nn.ELU()
        
        # Initialisation des poids
        self._initialize_weights()
        
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
        
    def forward(self, x):
        """
        Passe avant du r√©seau avec connexions r√©siduelles et activations vari√©es.
        
        Args:
            x (torch.Tensor): Tensor d'entr√©e
            
        Returns:
            torch.Tensor: Sortie du r√©seau (logits)
        """
        # Couche 1
        x = self.fc1(x)
        x = self.batch_norm1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # Couche 2 avec connexion r√©siduelle
        residual = x
        x = self.fc2(x)
        x = self.batch_norm2(x)
        x = self.leaky_relu(x)
        x = self.dropout(x)
        x = x + residual  # Connexion r√©siduelle
        
        # Couche 3
        x = self.fc3(x)
        x = self.batch_norm3(x)
        x = self.elu(x)
        x = self.dropout(x)
        
        # Couche 4
        x = self.fc4(x)
        x = self.batch_norm4(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # Couche de sortie
        x = self.fc5(x)
        return x

def print_progress(step, description):
    """
    Affiche une barre de progression format√©e.
    
    Args:
        step (int): Num√©ro de l'√©tape actuelle
        description (str): Description de l'√©tape
    """
    print(f"\n{'='*50}")
    print(f"üìã √âTAPE {step}: {description.upper()}")
    print(f"{'='*50}")

def print_configuration(config):
    """
    Affiche la configuration du mod√®le de fa√ßon format√©e.
    
    Args:
        config (dict): Dictionnaire de configuration
    """
    print("üîß Configuration du mod√®le:")
    for key, value in config.items():
        print(f"   {key}: {value}")
