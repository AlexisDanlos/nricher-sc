"""
Script principal pour l'entra√Ænement du mod√®le de classification e-commerce.
Ce script se concentre uniquement sur l'entra√Ænement et la sauvegarde du mod√®le.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np
import joblib
import pickle
import os
from datetime import datetime

# Import des modules locaux
from model_utils import TextClassifierNet, print_progress, print_configuration
from data_processing import load_excel_data, prepare_data_for_training, create_tfidf_vectorizers, prepare_features, prepare_labels

# === CONFIGURATION ===
FILE_PATH = "ecommerce_corrected_20250728_174305.xlsx"
LIBELLE_COL = "Libell√© produit"
NATURE_COL = "Nature"

# Configuration des donn√©es
LIMIT_ROWS = None  # Limiter pour les tests (ex: 20000), None pour tout charger

# Configuration du mod√®le
USE_GPU = True
USE_ENSEMBLE = False  # Pour les mod√®les CPU
BATCH_SIZE = 64
EPOCHS = 100
LEARNING_RATE = 0.001

# Configuration CUDA
if USE_GPU and torch.cuda.is_available():
    DEVICE = torch.device("cuda")
    print(f"üöÄ GPU activ√©: {torch.cuda.get_device_name(0)}")
    print(f"   M√©moire disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    DEVICE = torch.device("cpu")
    print("üñ•Ô∏è  Mode CPU activ√©")

def main():
    """Fonction principale d'entra√Ænement."""
    
    # === 1. CHARGEMENT DES DONN√âES ===
    print_progress(1, "Chargement des donn√©es")
    
    df = load_excel_data(FILE_PATH, LIBELLE_COL, NATURE_COL, max_rows=LIMIT_ROWS)
    
    # === 2. PR√âPARATION DES DONN√âES ===
    print_progress(2, "Pr√©paration des donn√©es")
    
    # Filtrage des cat√©gories rares pour am√©liorer les performances
    # Utilisation de min=2 pour permettre la stratification (train_test_split n√©cessite ‚â•2 √©chantillons/classe)
    min_samples_per_category = 1
    category_counts = df[NATURE_COL].value_counts()
    valid_categories = category_counts[category_counts >= min_samples_per_category].index
    
    # Filtration du dataset pour ne garder que les cat√©gories avec assez d'exemples
    df_filtered = df[df[NATURE_COL].isin(valid_categories)].copy()
    removed_count = len(df) - len(df_filtered)
    
    # Reset des indices pour assurer la continuit√©
    df_filtered = df_filtered.reset_index(drop=True)
    
    if removed_count > 0:
        print(f"‚ö†Ô∏è  {removed_count} produits supprim√©s (cat√©gories rares avec <{min_samples_per_category} exemples)")
        print(f"üìä Dataset d'entra√Ænement: {len(df_filtered)} produits, {len(valid_categories)} cat√©gories")
    
    print(f"üìä Cat√©gories avec ‚â•{min_samples_per_category} √©chantillons: {len(valid_categories)}")
    print(f"üéØ √âchantillons utilis√©s: {len(df_filtered)} / {len(df)}")
    print(f"üìà R√©partition: min={category_counts[valid_categories].min()}, max={category_counts[valid_categories].max()}, moyenne={category_counts[valid_categories].mean():.1f}")
    
    # === 3. CR√âATION DES FEATURES ===
    print_progress(3, "Cr√©ation des features TF-IDF")
    
    tfidf_configs = create_tfidf_vectorizers()
    X = prepare_features(df_filtered, LIBELLE_COL, tfidf_configs)
    y, le_filtered = prepare_labels(df_filtered, NATURE_COL)
    
    # === 4. DIVISION DES DONN√âES ===
    print_progress(4, "Division train/test")
    
    # V√©rification que toutes les classes ont au moins 2 √©chantillons pour la stratification
    unique, counts = np.unique(y, return_counts=True)
    min_class_count = counts.min()
    if min_class_count < 2:
        print(f"‚ö†Ô∏è  Stratification d√©sactiv√©e: certaines classes ont <2 √©chantillons (min={min_class_count})")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
    
    print(f"üìä Train: {X_train.shape[0]} √©chantillons")
    print(f"üìä Test: {X_test.shape[0]} √©chantillons")
    
    # === 5. ENTRA√éNEMENT DU MOD√àLE ===
    print_progress(5, "Entra√Ænement du mod√®le")
    
    if USE_GPU and torch.cuda.is_available():
        test_score = train_pytorch_model(X_train, X_test, y_train, y_test, tfidf_configs, le_filtered, valid_categories)
    else:
        test_score = train_cpu_model(X_train, X_test, y_train, y_test, le_filtered, valid_categories)
    
    print(f"üéØ Score final: {test_score:.4f}")
    print("üéâ Entra√Ænement termin√© avec succ√®s!")

def train_pytorch_model(X_train, X_test, y_train, y_test, tfidf_configs, le_filtered, valid_categories):
    """Entra√Æne le mod√®le PyTorch GPU - M√©thode exacte du backup."""
    
    # Configuration du mod√®le (m√™me que backup)
    input_size = X_train.shape[1]
    hidden_size = min(1024, input_size // 4)  # Architecture plus raisonnable pour GPU
    num_classes = len(le_filtered.classes_)
    
    # Utiliser des batches plus petits pour √©conomiser la m√©moire GPU
    batch_size = 64  # R√©duit pour √©viter OOM
    
    config = {
        "input_size": input_size,
        "hidden_size": hidden_size,
        "num_classes": num_classes,
        "batch_size": batch_size,
        "epochs": EPOCHS,
        "learning_rate": LEARNING_RATE,
        "device": DEVICE,
        "dropout_rate": 0.4
    }
    
    # Entra√Ænement d'un seul mod√®le optimis√© au lieu d'ensemble pour √©conomiser la m√©moire
    print(f"üî• Entra√Ænement mod√®le optimis√© sur GPU ({DEVICE}) avec {num_classes} cat√©gories...")
    print(f"üìä Architecture: {input_size} ‚Üí {hidden_size} ‚Üí {hidden_size//2} ‚Üí {hidden_size//4} ‚Üí {num_classes}")
    print(f"‚öôÔ∏è  Param√®tres: Batch={batch_size}, LR=0.001, Dropout=0.4, Label Smoothing=0.1")
    print(f"üéØ Objectif: Maximiser la pr√©cision (actuel: validation tous les 5 √©poques)")
    print("-" * 60)
    
    model = TextClassifierNet(
        input_size, 
        hidden_size, 
        num_classes, 
        dropout_rate=0.4
    ).to(DEVICE)
    
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=0.001, 
        weight_decay=1e-4,
        betas=(0.9, 0.999)
    )
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2
    )
    
    # Utiliser des batches plus petits pour √©conomiser la m√©moire GPU
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train).to(DEVICE),
        torch.LongTensor(y_train).to(DEVICE)
    )
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    
    # Pr√©parer les tenseurs de test
    X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)
    y_test_tensor = torch.LongTensor(y_test).to(DEVICE)
        
    # Entra√Ænement avanc√© avec un seul mod√®le optimis√©
    model.train()
    best_accuracy = 0
    patience_counter = 0
    best_model_state = None
    
    for epoch in range(100):  # Plus d'√©poques pour un seul mod√®le
        total_loss = 0
        batch_count = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            
            # Mixup data augmentation occasionnel
            if np.random.random() < 0.1 and len(data) > 1:
                lam = np.random.beta(0.2, 0.2)
                index = torch.randperm(data.size(0)).to(DEVICE)
                mixed_data = lam * data + (1 - lam) * data[index]
                output = model(mixed_data)
                loss = lam * criterion(output, target) + (1 - lam) * criterion(output, target[index])
            else:
                output = model(data)
                loss = criterion(output, target)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
            optimizer.step()
            total_loss += loss.item()
            batch_count += 1
            
            # Affichage du progr√®s par batch pour les premi√®res √©poques ou √©poques critiques
            if (epoch < 10 or epoch % 20 == 0) and batch_idx % (len(train_loader) // 4) == 0:
                progress_pct = (batch_idx + 1) / len(train_loader) * 100
                current_avg_loss = total_loss / (batch_idx + 1)
                print(f"     Batch {batch_idx+1:3d}/{len(train_loader)} ({progress_pct:5.1f}%) - Loss: {current_avg_loss:.4f}")
        
        scheduler.step()
        
        # Validation et affichage du progr√®s plus fr√©quent
        if epoch % 5 == 0 or epoch >= 65:  # Validation tous les 5 √©poques
            model.eval()
            with torch.no_grad():
                test_output = model(X_test_tensor)
                _, predicted = torch.max(test_output.data, 1)
                accuracy = (predicted == y_test_tensor).float().mean().item()
                
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    patience_counter = 0
                    # Sauvegarder le meilleur mod√®le
                    best_model_state = model.state_dict().copy()
                    improvement_indicator = "‚¨ÜÔ∏è"
                else:
                    patience_counter += 1
                    improvement_indicator = "‚û°Ô∏è"
                
                # Calcul du learning rate actuel
                current_lr = optimizer.param_groups[0]['lr']
                
                # Affichage d√©taill√© du progr√®s
                avg_loss = total_loss / batch_count
                print(f"   {improvement_indicator} √âpoque {epoch+1:3d}: Loss={avg_loss:.4f}, Acc={accuracy:.3f}, Best={best_accuracy:.3f}, LR={current_lr:.6f}, Patience={patience_counter}")
                
                if patience_counter >= 3 and epoch >= 45:  # Early stopping plus patient
                    print(f"   üõë Early stopping √† l'√©poque {epoch+1} (patience √©puis√©e)")
                    break
                    
            model.train()
        else:
            # Affichage rapide de la loss pour les autres √©poques
            avg_loss = total_loss / batch_count
            current_lr = optimizer.param_groups[0]['lr']
            print(f"   üìà √âpoque {epoch+1:3d}: Loss={avg_loss:.4f}, LR={current_lr:.6f}")
    
    # Restaurer le meilleur mod√®le
    if best_model_state:
        model.load_state_dict(best_model_state)
    print(f"   ‚úÖ Mod√®le termin√© - Meilleure pr√©cision: {best_accuracy:.3f}")
    
    # Wrapper simplifi√© pour un seul mod√®le
    class SingleModelWrapper:
        def __init__(self, model, device):
            self.model = model
            self.device = device
            
        def predict(self, X):
            # X est d√©j√† transform√© et combin√©
            X_tensor = torch.FloatTensor(X).to(self.device)
            
            self.model.eval()
            with torch.no_grad():
                output = self.model(X_tensor)
                _, predicted = torch.max(output.data, 1)
                return predicted.cpu().numpy()
    
    # Cr√©er le wrapper
    pipeline = SingleModelWrapper(model, DEVICE)
    
    # Score final
    final_predictions = pipeline.predict(X_test)
    test_score = (final_predictions == y_test).mean()
    
    # Sauvegarde du mod√®le
    save_pytorch_model(model, tfidf_configs, le_filtered, test_score, config, valid_categories, X_train, X_test)
    
    return test_score

def train_cpu_model(X_train, X_test, y_train, y_test, le_filtered, valid_categories):
    """Entra√Æne le mod√®le CPU (Random Forest ou Ensemble)."""
    
    if USE_ENSEMBLE:
        print("üå≤ Entra√Ænement du mod√®le ensemble...")
        
        # Cr√©ation des mod√®les de base
        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=30,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            oob_score=True,
            n_jobs=-1
        )
        
        lr = LogisticRegression(
            max_iter=1000,
            random_state=42,
            n_jobs=-1
        )
        
        # Ensemble voting
        ensemble = VotingClassifier(
            estimators=[('rf', rf), ('lr', lr)],
            voting='soft',
            n_jobs=-1
        )
        
        ensemble.fit(X_train, y_train)
        pipeline = ensemble
        
    else:
        print("üå≤ Entra√Ænement du Random Forest...")
        
        rf = RandomForestClassifier(
            n_estimators=300,
            max_depth=40,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            oob_score=True,
            n_jobs=-1
        )
        
        rf.fit(X_train, y_train)
        pipeline = rf
    
    # √âvaluation
    test_predicted = pipeline.predict(X_test)
    test_score = accuracy_score(y_test, test_predicted)
    print(f"‚úÖ Pr√©cision sur le test: {test_score:.4f}")
    
    # Sauvegarde
    save_cpu_model(pipeline, le_filtered, test_score, X_train, X_test)
    
    return test_score

def save_pytorch_model(model, tfidf_configs, le_filtered, test_score, config, valid_categories, X_train, X_test):
    """Sauvegarde le mod√®le PyTorch et ses composants."""
    
    print_progress(6, "Sauvegarde du mod√®le PyTorch")
    
    # Cr√©ation du dossier
    model_dir = "models"
    os.makedirs(model_dir, exist_ok=True)
    
    # G√©n√©ration des noms de fichiers
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    model_filename = f"{model_dir}/pytorch_model_{timestamp}.pth"
    tfidf_filename = f"{model_dir}/tfidf_configs_{timestamp}.pkl"
    label_encoder_filename = f"{model_dir}/label_encoder_{timestamp}.pkl"
    metadata_filename = f"{model_dir}/model_metadata_{timestamp}.pkl"
    
    try:
        # Sauvegarder le mod√®le PyTorch
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': config,
            'device': DEVICE,
            'test_score': test_score
        }, model_filename, _use_new_zipfile_serialization=False)
        
        # Sauvegarder les configurations TF-IDF
        with open(tfidf_filename, 'wb') as f:
            pickle.dump(tfidf_configs, f)
        
        # Sauvegarder le label encoder
        with open(label_encoder_filename, 'wb') as f:
            pickle.dump(le_filtered, f)
        
        # Sauvegarder les m√©tadonn√©es
        metadata = {
            'model_type': 'pytorch_gpu',
            'timestamp': timestamp,
            'test_score': test_score,
            'num_classes': config['num_classes'],
            'valid_categories': valid_categories.tolist(),
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'use_ensemble': False,
            'device': str(DEVICE)
        }
        with open(metadata_filename, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"‚úÖ Mod√®le PyTorch sauvegard√©:")
        print(f"   üìÅ Mod√®le: {model_filename}")
        print(f"   üìÅ TF-IDF: {tfidf_filename}")
        print(f"   üìÅ Encodeur: {label_encoder_filename}")
        print(f"   üìÅ M√©tadonn√©es: {metadata_filename}")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde: {e}")

def save_cpu_model(pipeline, le_filtered, test_score, X_train, X_test):
    """Sauvegarde le mod√®le CPU."""
    
    print_progress(6, "Sauvegarde du mod√®le CPU")
    
    # Cr√©ation du dossier
    model_dir = "models"
    os.makedirs(model_dir, exist_ok=True)
    
    # G√©n√©ration des noms de fichiers
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if USE_ENSEMBLE:
        model_filename = f"{model_dir}/ensemble_model_{timestamp}.pkl"
        model_type = 'ensemble_cpu'
    else:
        model_filename = f"{model_dir}/randomforest_model_{timestamp}.pkl"
        model_type = 'randomforest_cpu'
    
    label_encoder_filename = f"{model_dir}/label_encoder_{timestamp}.pkl"
    metadata_filename = f"{model_dir}/model_metadata_{timestamp}.pkl"
    
    try:
        # Sauvegarder le pipeline
        joblib.dump(pipeline, model_filename)
        
        # Sauvegarder le label encoder
        with open(label_encoder_filename, 'wb') as f:
            pickle.dump(le_filtered, f)
        
        # Sauvegarder les m√©tadonn√©es
        metadata = {
            'model_type': model_type,
            'timestamp': timestamp,
            'test_score': test_score,
            'num_classes': len(le_filtered.classes_),
            'valid_categories': [],  # CPU model doesn't use this but keep for compatibility
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'use_ensemble': USE_ENSEMBLE,
            'device': 'cpu'
        }
        with open(metadata_filename, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"‚úÖ Mod√®le CPU sauvegard√©:")
        print(f"   üìÅ Pipeline: {model_filename}")
        print(f"   üìÅ Encodeur: {label_encoder_filename}")
        print(f"   üìÅ M√©tadonn√©es: {metadata_filename}")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde: {e}")

if __name__ == "__main__":
    main()
