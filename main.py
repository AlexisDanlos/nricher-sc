"""
Script principal pour l'entra√Ænement du mod√®le de classification e-commerce.
Ce script se concentre uniquement sur l'entra√Ænement et la sauvegarde du mod√®le.
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import joblib
import pickle
import os
from datetime import datetime

# Import des modules locaux
from model_utils import TextClassifierNet, print_progress, print_configuration
from data_processing import load_excel_data, prepare_data_for_training, create_tfidf_vectorizers, prepare_features, prepare_labels

# === CONFIGURATION ===
FILE_PATH = "20210614 Ecommerce sales.xlsb"
LIBELLE_COL = "LIBELLE"
NATURE_COL = "NATURE"

# Configuration du mod√®le
USE_GPU = True
USE_ENSEMBLE = False  # Pour les mod√®les CPU
BATCH_SIZE = 64
EPOCHS = 100
LEARNING_RATE = 0.001

# Configuration CUDA
if USE_GPU and torch.cuda.is_available():
    DEVICE = torch.device("cuda")
    print(f"üöÄ GPU activ√©: {torch.cuda.get_device_name(0)}")
    print(f"   M√©moire disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    DEVICE = torch.device("cpu")
    print("üñ•Ô∏è  Mode CPU activ√©")

def main():
    """Fonction principale d'entra√Ænement."""
    
    # === 1. CHARGEMENT DES DONN√âES ===
    print_progress(1, "Chargement des donn√©es")
    
    df = load_excel_data(FILE_PATH, LIBELLE_COL, NATURE_COL)
    
    # === 2. PR√âPARATION DES DONN√âES ===
    print_progress(2, "Pr√©paration des donn√©es")
    
    df_filtered, valid_categories, category_counts = prepare_data_for_training(
        df, LIBELLE_COL, NATURE_COL, min_samples=30
    )
    
    # === 3. CR√âATION DES FEATURES ===
    print_progress(3, "Cr√©ation des features TF-IDF")
    
    tfidf_configs = create_tfidf_vectorizers()
    X = prepare_features(df_filtered, LIBELLE_COL, tfidf_configs)
    y, le_filtered = prepare_labels(df_filtered, NATURE_COL)
    
    # === 4. DIVISION DES DONN√âES ===
    print_progress(4, "Division train/test")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"üìä Train: {X_train.shape[0]} √©chantillons")
    print(f"üìä Test: {X_test.shape[0]} √©chantillons")
    
    # === 5. ENTRA√éNEMENT DU MOD√àLE ===
    print_progress(5, "Entra√Ænement du mod√®le")
    
    if USE_GPU and torch.cuda.is_available():
        test_score = train_pytorch_model(X_train, X_test, y_train, y_test, tfidf_configs, le_filtered)
    else:
        test_score = train_cpu_model(X_train, X_test, y_train, y_test, le_filtered)
    
    print(f"üéØ Score final: {test_score:.4f}")
    print("üéâ Entra√Ænement termin√© avec succ√®s!")

def train_pytorch_model(X_train, X_test, y_train, y_test, tfidf_configs, le_filtered):
    """Entra√Æne le mod√®le PyTorch GPU."""
    
    # Configuration du mod√®le
    input_size = X_train.shape[1]
    hidden_size = 1024
    num_classes = len(le_filtered.classes_)
    
    config = {
        "input_size": input_size,
        "hidden_size": hidden_size,
        "num_classes": num_classes,
        "batch_size": BATCH_SIZE,
        "epochs": EPOCHS,
        "learning_rate": LEARNING_RATE,
        "device": DEVICE
    }
    print_configuration(config)
    
    # Cr√©ation du mod√®le
    model = TextClassifierNet(input_size, hidden_size, num_classes).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # Pr√©paration des donn√©es
    X_train_tensor = torch.FloatTensor(X_train).to(DEVICE)
    y_train_tensor = torch.LongTensor(y_train).to(DEVICE)
    X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)
    y_test_tensor = torch.LongTensor(y_test).to(DEVICE)
    
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    # Entra√Ænement
    print("üî• D√©but de l'entra√Ænement...")
    model.train()
    
    for epoch in range(EPOCHS):
        total_loss = 0
        correct_predictions = 0
        total_samples = 0
        
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_samples += batch_y.size(0)
            correct_predictions += (predicted == batch_y).sum().item()
        
        if (epoch + 1) % 10 == 0:
            avg_loss = total_loss / len(train_loader)
            accuracy = 100 * correct_predictions / total_samples
            print(f"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
    
    # √âvaluation
    print("üìä √âvaluation du mod√®le...")
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        _, test_predicted = torch.max(test_outputs.data, 1)
        test_predicted_cpu = test_predicted.cpu().numpy()
    
    test_score = accuracy_score(y_test, test_predicted_cpu)
    print(f"‚úÖ Pr√©cision sur le test: {test_score:.4f}")
    
    # Sauvegarde du mod√®le
    save_pytorch_model(model, tfidf_configs, le_filtered, test_score, config)
    
    return test_score

def train_cpu_model(X_train, X_test, y_train, y_test, le_filtered):
    """Entra√Æne le mod√®le CPU (Random Forest ou Ensemble)."""
    
    if USE_ENSEMBLE:
        print("üå≤ Entra√Ænement du mod√®le ensemble...")
        
        # Cr√©ation des mod√®les de base
        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=30,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            oob_score=True,
            n_jobs=-1
        )
        
        lr = LogisticRegression(
            max_iter=1000,
            random_state=42,
            n_jobs=-1
        )
        
        # Ensemble voting
        ensemble = VotingClassifier(
            estimators=[('rf', rf), ('lr', lr)],
            voting='soft',
            n_jobs=-1
        )
        
        ensemble.fit(X_train, y_train)
        pipeline = ensemble
        
    else:
        print("üå≤ Entra√Ænement du Random Forest...")
        
        rf = RandomForestClassifier(
            n_estimators=300,
            max_depth=40,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            oob_score=True,
            n_jobs=-1
        )
        
        rf.fit(X_train, y_train)
        pipeline = rf
    
    # √âvaluation
    test_predicted = pipeline.predict(X_test)
    test_score = accuracy_score(y_test, test_predicted)
    print(f"‚úÖ Pr√©cision sur le test: {test_score:.4f}")
    
    # Sauvegarde
    save_cpu_model(pipeline, le_filtered, test_score)
    
    return test_score

def save_pytorch_model(model, tfidf_configs, le_filtered, test_score, config):
    """Sauvegarde le mod√®le PyTorch et ses composants."""
    
    print_progress(6, "Sauvegarde du mod√®le PyTorch")
    
    # Cr√©ation du dossier
    model_dir = "models"
    os.makedirs(model_dir, exist_ok=True)
    
    # G√©n√©ration des noms de fichiers
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    model_filename = f"{model_dir}/pytorch_model_{timestamp}.pth"
    tfidf_filename = f"{model_dir}/tfidf_configs_{timestamp}.pkl"
    label_encoder_filename = f"{model_dir}/label_encoder_{timestamp}.pkl"
    metadata_filename = f"{model_dir}/model_metadata_{timestamp}.pkl"
    
    try:
        # Sauvegarder le mod√®le PyTorch
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': config,
            'device': DEVICE,
            'test_score': test_score
        }, model_filename, _use_new_zipfile_serialization=False)
        
        # Sauvegarder les configurations TF-IDF
        with open(tfidf_filename, 'wb') as f:
            pickle.dump(tfidf_configs, f)
        
        # Sauvegarder le label encoder
        with open(label_encoder_filename, 'wb') as f:
            pickle.dump(le_filtered, f)
        
        # Sauvegarder les m√©tadonn√©es
        metadata = {
            'model_type': 'pytorch_gpu',
            'timestamp': timestamp,
            'test_score': test_score,
            'num_classes': config['num_classes'],
            'device': str(DEVICE)
        }
        with open(metadata_filename, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"‚úÖ Mod√®le PyTorch sauvegard√©:")
        print(f"   üìÅ Mod√®le: {model_filename}")
        print(f"   üìÅ TF-IDF: {tfidf_filename}")
        print(f"   üìÅ Encodeur: {label_encoder_filename}")
        print(f"   üìÅ M√©tadonn√©es: {metadata_filename}")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde: {e}")

def save_cpu_model(pipeline, le_filtered, test_score):
    """Sauvegarde le mod√®le CPU."""
    
    print_progress(6, "Sauvegarde du mod√®le CPU")
    
    # Cr√©ation du dossier
    model_dir = "models"
    os.makedirs(model_dir, exist_ok=True)
    
    # G√©n√©ration des noms de fichiers
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if USE_ENSEMBLE:
        model_filename = f"{model_dir}/ensemble_model_{timestamp}.pkl"
        model_type = 'ensemble_cpu'
    else:
        model_filename = f"{model_dir}/randomforest_model_{timestamp}.pkl"
        model_type = 'randomforest_cpu'
    
    label_encoder_filename = f"{model_dir}/label_encoder_{timestamp}.pkl"
    metadata_filename = f"{model_dir}/model_metadata_{timestamp}.pkl"
    
    try:
        # Sauvegarder le pipeline
        joblib.dump(pipeline, model_filename)
        
        # Sauvegarder le label encoder
        with open(label_encoder_filename, 'wb') as f:
            pickle.dump(le_filtered, f)
        
        # Sauvegarder les m√©tadonn√©es
        metadata = {
            'model_type': model_type,
            'timestamp': timestamp,
            'test_score': test_score,
            'num_classes': len(le_filtered.classes_),
            'use_ensemble': USE_ENSEMBLE,
            'device': 'cpu'
        }
        with open(metadata_filename, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"‚úÖ Mod√®le CPU sauvegard√©:")
        print(f"   üìÅ Pipeline: {model_filename}")
        print(f"   üìÅ Encodeur: {label_encoder_filename}")
        print(f"   üìÅ M√©tadonn√©es: {metadata_filename}")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde: {e}")

if __name__ == "__main__":
    main()
