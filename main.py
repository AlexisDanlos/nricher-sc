import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
import time
import os
from datetime import datetime
from numpy import float32
import numpy as np
import pickle
import joblib

# === CONFIGURATION GPU/CPU ===
USE_GPU = True  # Mettre True pour essayer d'utiliser le GPU (n√©cessite des d√©pendances suppl√©mentaires)
USE_ENSEMBLE = True  # Utiliser plusieurs mod√®les pour am√©liorer la pr√©cision

if USE_GPU:
    try:
        print("üî• Tentative d'importation des biblioth√®ques GPU")
        # Tentative d'importation des biblioth√®ques GPU
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import DataLoader, TensorDataset
        from sklearn.linear_model import SGDClassifier
        print("üî• Mode GPU activ√© - Utilisation de PyTorch")
        DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üéØ Device d√©tect√©: {DEVICE}")
        
        # Classe de r√©seau de neurones avanc√© pour GPU
        class TextClassifierNet(nn.Module):
            def __init__(self, input_size, hidden_size, num_classes, dropout_rate=0.4):
                super(TextClassifierNet, self).__init__()
                # Architecture plus profonde et sophistiqu√©e
                self.fc1 = nn.Linear(input_size, hidden_size)
                self.fc2 = nn.Linear(hidden_size, hidden_size)
                self.fc3 = nn.Linear(hidden_size, hidden_size // 2)
                self.fc4 = nn.Linear(hidden_size // 2, hidden_size // 4)
                self.fc5 = nn.Linear(hidden_size // 4, num_classes)
                
                # Normalisation et r√©gularisation
                self.dropout = nn.Dropout(dropout_rate)
                self.batch_norm1 = nn.BatchNorm1d(hidden_size)
                self.batch_norm2 = nn.BatchNorm1d(hidden_size)
                self.batch_norm3 = nn.BatchNorm1d(hidden_size // 2)
                self.batch_norm4 = nn.BatchNorm1d(hidden_size // 4)
                
                # Fonctions d'activation vari√©es
                self.relu = nn.ReLU()
                self.leaky_relu = nn.LeakyReLU(0.1)
                self.elu = nn.ELU()
                
                # Initialisation des poids
                self._initialize_weights()
                
            def _initialize_weights(self):
                for m in self.modules():
                    if isinstance(m, nn.Linear):
                        nn.init.xavier_uniform_(m.weight)
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                
            def forward(self, x):
                # Couche 1
                x = self.fc1(x)
                x = self.batch_norm1(x)
                x = self.relu(x)
                x = self.dropout(x)
                
                # Couche 2 avec connexion r√©siduelle
                residual = x
                x = self.fc2(x)
                x = self.batch_norm2(x)
                x = self.leaky_relu(x)
                x = self.dropout(x)
                x = x + residual  # Connexion r√©siduelle
                
                # Couche 3
                x = self.fc3(x)
                x = self.batch_norm3(x)
                x = self.elu(x)
                x = self.dropout(x)
                
                # Couche 4
                x = self.fc4(x)
                x = self.batch_norm4(x)
                x = self.relu(x)
                x = self.dropout(x)
                
                # Couche de sortie
                x = self.fc5(x)
                return x
                
    except ImportError:
        print("‚ö†Ô∏è  Biblioth√®ques GPU non trouv√©es - Utilisation du CPU")
        print("üí° Pour installer GPU: pip install torch torchvision")
        USE_GPU = False

# === CONFIGURATION DE PERFORMANCE ===
# Utilisation de tous les c≈ìurs CPU disponibles
os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())
os.environ['MKL_NUM_THREADS'] = str(os.cpu_count())

# Configuration pour joblib (parall√©lisation scikit-learn)
from joblib import parallel_backend

def print_progress(step, message):
    """Affiche le progr√®s avec un indicateur visuel"""
    print(f"[{step}/8] {message}...")

print("üöÄ D√©marrage de l'analyse e-commerce")
print("=" * 50)

# === INFORMATION GPU ===
if not USE_GPU:
    print("üí° Pour activer l'acc√©l√©ration GPU:")
    print("   1. Installer PyTorch: pip install torch torchvision")
    print("   2. Changer USE_GPU = True dans le script")
    print("   üî• Le GPU utilisera un r√©seau de neurones PyTorch (meilleure pr√©cision)")
    print("   ÔøΩ CPU utilise RandomForest (tr√®s bonne pr√©cision)")
    print("-" * 50)

# === 1. CHARGEMENT DES DONN√âES ===
print_progress(1, "Chargement des donn√©es")

# Option pour limiter le nombre de lignes (utile pour les tests)
LIMIT_ROWS = None  # Limiter pour √©viter les probl√®mes de m√©moire GPU

xlsb_path = "ecommerce_corrected_20250728_174305.xlsx"
NATURE_COL = "Nature"  # Colonne des cat√©gories
LIBELLE_COL = "Libell√© produit"  # Colonne des libell√©s de produits
SHEET_NAME = "Sheet1"  # Nom de la feuille par d√©faut
# SHEET_NAME = "20210614 Ecommerce sales"
if LIMIT_ROWS:
    # Premi√®re lecture pour conna√Ætre toutes les cat√©gories
    df_sample = pd.read_excel(xlsb_path, sheet_name=SHEET_NAME, engine="calamine", nrows=1000)
    df_sample = df_sample[[LIBELLE_COL, NATURE_COL]].dropna()
    all_categories = df_sample[NATURE_COL].unique()

    # Chargement limit√©
    df = pd.read_excel(xlsb_path, sheet_name=SHEET_NAME, engine="calamine", nrows=LIMIT_ROWS)
    df = df[[LIBELLE_COL, NATURE_COL]].dropna()

    # V√©rification et ajout des cat√©gories manquantes
    current_categories = df[NATURE_COL].unique()
    missing_categories = set(all_categories) - set(current_categories)
    
    if missing_categories:
        print(f"‚ö†Ô∏è  Cat√©gories manquantes d√©tect√©es: {len(missing_categories)}")
        # Rechargement du fichier pour r√©cup√©rer les cat√©gories manquantes
        df_full = pd.read_excel(xlsb_path, sheet_name=SHEET_NAME, engine="calamine")
        df_full = df_full[[LIBELLE_COL, NATURE_COL]].dropna()

        # Ajout d'exemples de chaque cat√©gorie manquante
        for category in missing_categories:
            category_samples = df_full[df_full[NATURE_COL] == category].head(5)  # 5 exemples par cat√©gorie
            df = pd.concat([df, category_samples], ignore_index=True)
        
        print(f"‚úÖ Ajout de {len(missing_categories)} cat√©gories manquantes")
    
    print(f"‚ö†Ô∏è  Mode test: chargement limit√© √† {LIMIT_ROWS} lignes + toutes les cat√©gories")
else:
    df = pd.read_excel(xlsb_path, sheet_name=SHEET_NAME, engine="calamine")
    df = df[[LIBELLE_COL, NATURE_COL]].dropna()
    print("üìä Chargement complet du fichier")

print(f"‚úÖ Donn√©es charg√©es: {len(df)} produits trouv√©s avec {len(df[NATURE_COL].unique())} cat√©gories uniques")

# Reset des indices apr√®s toutes les op√©rations de chargement
df = df.reset_index(drop=True)

# === 2. NETTOYAGE DES TEXTES SIMPLE ET EFFICACE ===
print_progress(2, "Nettoyage des textes")

def clean_text(text):
    """Nettoyage am√©lior√© qui pr√©serve les dimensions cruciales pour la classification"""
    text = str(text).lower()
    
    # Pr√©servation des patterns importants AVANT nettoyage
    # Remplacement des caract√®res sp√©ciaux par des mots-cl√©s
    text = re.sub(r'&', ' et ', text)
    text = re.sub(r'%', ' pourcent ', text)
    text = re.sub(r'\+', ' plus ', text)
    text = re.sub(r'@', ' arobase ', text)
    
    # PR√âSERVATION INTELLIGENTE DES DIMENSIONS (traitement s√©quentiel pour √©viter les conflits)
    # D'abord traiter les 3D, puis marquer les zones trait√©es, puis traiter les 2D restants
    
    # √âtape 1: Marquer temporairement les dimensions 3D
    text = re.sub(r'(\d+(?:[,\.]\d+)?)\s*[xX√ó*]\s*(\d+(?:[,\.]\d+)?)\s*[xX√ó*]\s*(\d+(?:[,\.]\d+)?)(?:\s*cm)?', 
                  r' DIM3D_\1x\2x\3 ', text)
    
    # √âtape 2: Traiter les dimensions 2D restantes (qui ne font pas partie d'une 3D)
    text = re.sub(r'(\d+(?:[,\.]\d+)?)\s*[xX√ó*]\s*(\d+(?:[,\.]\d+)?)(?:\s*cm)?', 
                  r' dim_\1x\2 ', text)
    
    # √âtape 3: Restaurer les dimensions 3D avec le bon pr√©fixe
    text = re.sub(r'DIM3D_(\d+(?:[,\.]\d+)?x\d+(?:[,\.]\d+)?x\d+(?:[,\.]\d+)?)', 
                  r'dim_\1', text)
    
    # Pr√©servation des tailles de v√™tements
    text = re.sub(r'\b(xs|s|m|l|xl|xxl|xxxl)\b', r' taille_\1 ', text)
    
    # Pr√©servation des mesures avec unit√©s (APR√àS traitement des dimensions principales)
    text = re.sub(r'\b(\d+(?:[,\.]\d+)?)\s*(cm|mm|m|kg|g|ml|l)\b', r' mesure_\1_\2 ', text)
    
    # Normalisation des points et virgules dans les dimensions preserv√©es
    text = re.sub(r'dim_(\d+)[,\.](\d+)', r'dim_\1point\2', text)
    
    # Nettoyage des mots orphelins comme "x" restants
    text = re.sub(r'\s+[xX√ó*]\s+', ' ', text)
    
    # Nettoyage des caract√®res sp√©ciaux mais pr√©servation des espaces et underscores
    text = re.sub(r'[^\w\s_]', ' ', text)
    
    # Suppression des mots tr√®s courts qui n'apportent pas d'information (SAUF les dimensions)
    words = text.split()
    words = [word for word in words if len(word) >= 2 or word.startswith('dim_')]
    
    # Normalisation des espaces
    text = ' '.join(words)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# Application du nettoyage simple
df["clean_libelle"] = df[LIBELLE_COL].apply(clean_text)
print("‚úÖ Textes nettoy√©s")

# === 3. ENCODAGE DES CAT√âGORIES ===
print_progress(3, "Encodage des cat√©gories")
le = LabelEncoder()
df["nature_encoded"] = le.fit_transform(df[NATURE_COL])
print(f"‚úÖ Cat√©gories encod√©es: {len(le.classes_)} cat√©gories uniques")

# === 4. ENTRA√éNEMENT DU MOD√àLE SIMPLE ET RAPIDE ===
print_progress(4, "Division des donn√©es et entra√Ænement du mod√®le")

# Filtrage des cat√©gories rares pour am√©liorer les performances
min_samples_per_category = 1  # Augment√© pour de meilleures performances
category_counts = df[NATURE_COL].value_counts()
valid_categories = category_counts[category_counts >= min_samples_per_category].index

# Filtration du dataset pour ne garder que les cat√©gories avec assez d'exemples
df_filtered = df[df[NATURE_COL].isin(valid_categories)].copy()
removed_count = len(df) - len(df_filtered)

# Reset des indices pour assurer la continuit√©
df_filtered = df_filtered.reset_index(drop=True)

if removed_count > 0:
    print(f"‚ö†Ô∏è  {removed_count} produits supprim√©s (cat√©gories rares avec <{min_samples_per_category} exemples)")
    print(f"üìä Dataset d'entra√Ænement: {len(df_filtered)} produits, {len(valid_categories)} cat√©gories")
    
    # Re-encodage des cat√©gories filtr√©es
    le_filtered = LabelEncoder()
    df_filtered["nature_encoded"] = le_filtered.fit_transform(df_filtered[NATURE_COL])

    # Division des donn√©es avec stratification possible
    X_train, X_test, y_train, y_test = train_test_split(
        df_filtered["clean_libelle"], df_filtered["nature_encoded"], 
        test_size=0.2, random_state=42, stratify=df_filtered["nature_encoded"]
    )
    
else:
    print("üìä Toutes les cat√©gories ont assez d'exemples")
    le_filtered = le
    X_train, X_test, y_train, y_test = train_test_split(
        df["clean_libelle"], df["nature_encoded"], test_size=0.2, random_state=42
    )

# Configuration optimis√©e pour la performance
print(f"üöÄ Utilisation de {os.cpu_count()} c≈ìurs CPU pour l'acc√©l√©ration")

if USE_GPU and 'torch' in globals():
    # Pipeline GPU-optimis√© avec r√©seau de neurones PyTorch + Ensemble
    print("‚ö° Configuration GPU: R√©seau de neurones PyTorch haute pr√©cision + Ensemble")
    
    # TF-IDF optimis√© pour meilleure pr√©cision (r√©duit pour GPU)
    tfidf_configs = [
        # Configuration 1: Features g√©n√©rales (r√©duit)
        TfidfVectorizer(
            max_features=4000,  # R√©duit pour √©viter OOM
            ngram_range=(1, 2),  # R√©duit les n-grams
            min_df=3,
            max_df=0.9,
            dtype=float32,
            sublinear_tf=True,
            analyzer='word',
            token_pattern=r'\b[a-zA-Z]{2,}\b',
            stop_words=None,
            use_idf=True,
            smooth_idf=True,
            norm='l2'
        ),
        # Configuration 2: Focus sur les caract√®res (r√©duit)
        TfidfVectorizer(
            max_features=2000,  # R√©duit pour √©viter OOM
            ngram_range=(2, 3),  # R√©duit les n-grams
            min_df=5,
            max_df=0.85,
            dtype=float32,
            sublinear_tf=True,
            analyzer='char',
            stop_words=None,
            use_idf=True,
            smooth_idf=True,
            norm='l2'
        )
    ]
    
    # Entra√Ænement de plusieurs TF-IDF
    print("üìä Calcul des features TF-IDF multi-configurations...")
    X_train_features = []
    X_test_features = []
    
    for i, tfidf in enumerate(tfidf_configs):
        print(f"   Configuration {i+1}: {tfidf.analyzer} n-grams {tfidf.ngram_range}")
        X_train_tfidf = tfidf.fit_transform(X_train).toarray()
        X_test_tfidf = tfidf.transform(X_test).toarray()
        X_train_features.append(X_train_tfidf)
        X_test_features.append(X_test_tfidf)
    
    # Combinaison des features
    import numpy as np
    X_train_combined = np.hstack(X_train_features)
    X_test_combined = np.hstack(X_test_features)
    print(f"   Features combin√©es: {X_train_combined.shape[1]} dimensions")
    
    # Configuration du mod√®le PyTorch optimis√© pour GPU
    input_size = X_train_combined.shape[1]
    hidden_size = min(1024, input_size // 4)  # Architecture plus raisonnable pour GPU
    num_classes = len(le_filtered.classes_)
    
    # Utiliser des batches plus petits pour √©conomiser la m√©moire GPU
    batch_size = 64  # R√©duit pour √©viter OOM
    
    # Entra√Ænement d'un seul mod√®le optimis√© au lieu d'ensemble pour √©conomiser la m√©moire
    print(f"üî• Entra√Ænement mod√®le optimis√© sur GPU ({DEVICE}) avec {num_classes} cat√©gories...")
    print(f"üìä Architecture: {input_size} ‚Üí {hidden_size} ‚Üí {hidden_size//2} ‚Üí {hidden_size//4} ‚Üí {num_classes}")
    print(f"‚öôÔ∏è  Param√®tres: Batch={batch_size}, LR=0.001, Dropout=0.4, Label Smoothing=0.1")
    print(f"üéØ Objectif: Maximiser la pr√©cision (actuel: validation tous les 5 √©poques)")
    print("-" * 60)
    
    model = TextClassifierNet(
        input_size, 
        hidden_size, 
        num_classes, 
        dropout_rate=0.4
    ).to(DEVICE)
    
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=0.001, 
        weight_decay=1e-4,
        betas=(0.9, 0.999)
    )
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2
    )
    
    # Utiliser des batches plus petits pour √©conomiser la m√©moire GPU
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train_combined).to(DEVICE),
        torch.LongTensor(y_train.values).to(DEVICE)
    )
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    
    # Pr√©parer les tenseurs de test
    X_test_tensor = torch.FloatTensor(X_test_combined).to(DEVICE)
    y_test_tensor = torch.LongTensor(y_test.values).to(DEVICE)
        
    # Entra√Ænement avanc√© avec un seul mod√®le optimis√©
    model.train()
    best_accuracy = 0
    patience_counter = 0
    best_model_state = None
    
    for epoch in range(100):  # Plus d'√©poques pour un seul mod√®le
        total_loss = 0
        batch_count = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            
            # Mixup data augmentation occasionnel
            if np.random.random() < 0.1 and len(data) > 1:
                lam = np.random.beta(0.2, 0.2)
                index = torch.randperm(data.size(0)).to(DEVICE)
                mixed_data = lam * data + (1 - lam) * data[index]
                output = model(mixed_data)
                loss = lam * criterion(output, target) + (1 - lam) * criterion(output, target[index])
            else:
                output = model(data)
                loss = criterion(output, target)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
            optimizer.step()
            total_loss += loss.item()
            batch_count += 1
            
            # Affichage du progr√®s par batch pour les premi√®res √©poques ou √©poques critiques
            if (epoch < 10 or epoch % 20 == 0) and batch_idx % (len(train_loader) // 4) == 0:
                progress_pct = (batch_idx + 1) / len(train_loader) * 100
                current_avg_loss = total_loss / (batch_idx + 1)
                print(f"     Batch {batch_idx+1:3d}/{len(train_loader)} ({progress_pct:5.1f}%) - Loss: {current_avg_loss:.4f}")
        
        scheduler.step()
        
        # Validation et affichage du progr√®s plus fr√©quent
        if epoch % 5 == 0 or epoch >= 70:  # Validation tous les 5 √©poques
            model.eval()
            with torch.no_grad():
                test_output = model(X_test_tensor)
                _, predicted = torch.max(test_output.data, 1)
                accuracy = (predicted == y_test_tensor).float().mean().item()
                
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    patience_counter = 0
                    # Sauvegarder le meilleur mod√®le
                    best_model_state = model.state_dict().copy()
                    improvement_indicator = "‚¨ÜÔ∏è"
                else:
                    patience_counter += 1
                    improvement_indicator = "‚û°Ô∏è"
                
                # Calcul du learning rate actuel
                current_lr = optimizer.param_groups[0]['lr']
                
                # Affichage d√©taill√© du progr√®s
                avg_loss = total_loss / batch_count
                print(f"   {improvement_indicator} √âpoque {epoch+1:3d}: Loss={avg_loss:.4f}, Acc={accuracy:.3f}, Best={best_accuracy:.3f}, LR={current_lr:.6f}, Patience={patience_counter}")
                
                if patience_counter >= 3 and epoch >= 35:  # Early stopping plus patient
                    print(f"   üõë Early stopping √† l'√©poque {epoch+1} (patience √©puis√©e)")
                    break
                    
            model.train()
        else:
            # Affichage rapide de la loss pour les autres √©poques
            avg_loss = total_loss / batch_count
            current_lr = optimizer.param_groups[0]['lr']
            print(f"   üìà √âpoque {epoch+1:3d}: Loss={avg_loss:.4f}, LR={current_lr:.6f}")
    
    # Restaurer le meilleur mod√®le
    if best_model_state:
        model.load_state_dict(best_model_state)
    print(f"   ‚úÖ Mod√®le termin√© - Meilleure pr√©cision: {best_accuracy:.3f}")
    
    # Wrapper simplifi√© pour un seul mod√®le
    class SingleModelWrapper:
        def __init__(self, model, tfidf_configs, device):
            self.model = model
            self.tfidf_configs = tfidf_configs
            self.device = device
            
        def predict(self, X):
            # Pr√©parer les features pour chaque configuration TF-IDF
            X_features = []
            for tfidf in self.tfidf_configs:
                X_tfidf = tfidf.transform(X).toarray()
                X_features.append(X_tfidf)
            
            X_combined = np.hstack(X_features)
            X_tensor = torch.FloatTensor(X_combined).to(self.device)
            
            self.model.eval()
            with torch.no_grad():
                output = self.model(X_tensor)
                _, predicted = torch.max(output.data, 1)
                return predicted.cpu().numpy()
    
    # Cr√©er le wrapper
    pipeline = SingleModelWrapper(model, tfidf_configs, DEVICE)
    
    # Score final
    final_predictions = pipeline.predict(X_test)
    test_score = (final_predictions == y_test.values).mean()
    
else:
    # Pipeline CPU-optimis√© avec ensemble (RandomForest + XGBoost + SVM)
    print("üîß Configuration CPU: Ensemble RandomForest + Mod√®les additionnels")
    
    if USE_ENSEMBLE:
        # Configuration TF-IDF optimis√©e pour ensemble
        tfidf_ensemble = TfidfVectorizer(
            max_features=7000,  # Plus de features pour l'ensemble
            ngram_range=(1, 3),  # Trigrams pour plus de contexte
            min_df=2,
            max_df=0.9,
            dtype=float32,
            sublinear_tf=True,
            analyzer='word',
            token_pattern=r'\b[a-zA-Z]{2,}\b',
            stop_words=None,
            use_idf=True,
            smooth_idf=True,
            norm='l2'
        )
        
        # Calcul des features
        print("üìä Calcul des features TF-IDF pour ensemble...")
        X_train_tfidf = tfidf_ensemble.fit_transform(X_train)
        X_test_tfidf = tfidf_ensemble.transform(X_test)
        
        from sklearn.svm import SVC
        from sklearn.naive_bayes import MultinomialNB
        from sklearn.linear_model import LogisticRegression
        from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier
        
        # Cr√©ation de plusieurs mod√®les
        models_ensemble = [
            ("rf", RandomForestClassifier(
                n_estimators=800,  # Plus d'arbres
                random_state=42,
                n_jobs=-1,
                max_depth=None,
                min_samples_split=2,
                min_samples_leaf=1,
                max_features='sqrt',
                bootstrap=True,
                oob_score=True,
                class_weight='balanced',
                criterion='gini'
            )),
            ("et", ExtraTreesClassifier(
                n_estimators=600,
                random_state=42,
                n_jobs=-1,
                max_depth=None,
                min_samples_split=3,
                min_samples_leaf=1,
                max_features='sqrt',
                bootstrap=True,
                oob_score=True,
                class_weight='balanced',
                criterion='gini'
            )),
            ("lr", LogisticRegression(
                max_iter=1000,
                random_state=42,
                n_jobs=-1,
                C=1.0,
                class_weight='balanced',
                solver='saga',
                penalty='elasticnet',
                l1_ratio=0.5
            )),
            ("nb", MultinomialNB(
                alpha=0.1,
                fit_prior=True,
                class_prior=None
            ))
        ]
        
        print(f"üöÄ Entra√Ænement ensemble de {len(models_ensemble)} mod√®les CPU...")
        
        # Voting classifier avec soft voting
        ensemble_classifier = VotingClassifier(
            estimators=models_ensemble,
            voting='soft',  # Utilise les probabilit√©s
            n_jobs=-1
        )
        
        # Pipeline complet
        pipeline = Pipeline([
            ("tfidf", tfidf_ensemble),
            ("ensemble", ensemble_classifier)
        ])
        
        # Entra√Ænement avec parall√©lisation
        with parallel_backend('threading', n_jobs=-1):
            pipeline.fit(X_train, y_train)
            
        test_score = pipeline.score(X_test, y_test)
        
        # Affichage des scores individuels
        print("üìä Scores individuels des mod√®les:")
        X_train_tfidf_full = tfidf_ensemble.fit_transform(X_train)
        X_test_tfidf_full = tfidf_ensemble.transform(X_test)
        
        for name, model in models_ensemble:
            model.fit(X_train_tfidf_full, y_train)
            individual_score = model.score(X_test_tfidf_full, y_test)
            print(f"   {name}: {individual_score:.3f}")
        
    else:
        # Configuration standard si ensemble d√©sactiv√©
        pipeline = Pipeline([
            ("tfidf", TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.95,
                dtype=float32,
                sublinear_tf=True,
                analyzer='word',
                token_pattern=r'\b[a-zA-Z]{2,}\b',
                stop_words=None,
                use_idf=True,
                smooth_idf=True,
                norm='l2'
            )),
            ("clf", RandomForestClassifier(
                n_estimators=500,
                random_state=42,
                n_jobs=-1,
                max_depth=None,
                min_samples_split=2,
                min_samples_leaf=1,
                max_features='sqrt',
                bootstrap=True,
                oob_score=True,
                class_weight='balanced',
                criterion='gini'
            ))
        ])
        
        # Entra√Ænement CPU classique
        with parallel_backend('threading', n_jobs=-1):
            pipeline.fit(X_train, y_train)
        
        test_score = pipeline.score(X_test, y_test)

# Entra√Ænement adapt√© selon la configuration
start_time = time.time()

# Le mod√®le est d√©j√† entra√Æn√© dans la section pr√©c√©dente
training_time = time.time() - start_time

# Scores de validation adapt√©s selon le type de mod√®le
if USE_GPU and 'torch' in globals():
    # PyTorch GPU optimis√©
    print(f"‚úÖ Mod√®le GPU (PyTorch optimis√©) entra√Æn√© en {training_time:.2f}s")
    print(f"üìä Score test: {test_score:.3f}")
    print(f"üìä Mod√®le: R√©seau de neurones PyTorch avanc√© (GPU)")
    print(f" Train: {len(X_train)}, Test: {len(X_test)}")
else:
    # CPU Ensemble ou standard
    if USE_ENSEMBLE:
        print(f"‚úÖ Ensemble CPU entra√Æn√© en {training_time:.2f}s")
        print(f"üìä Score test ensemble: {test_score:.3f}")
        print(f"üìä Mod√®le: Ensemble de {len(models_ensemble)} mod√®les (CPU)")
        print(f" Train: {len(X_train)}, Test: {len(X_test)}")
    else:
        oob_score = pipeline.named_steps['clf'].oob_score_
        print(f"‚úÖ Mod√®le CPU (RandomForest) entra√Æn√© en {training_time:.2f}s")
        print(f"üìä Score test: {test_score:.3f}")
        print(f"üìä Score OOB: {oob_score:.3f}")
        print(f" Train: {len(X_train)}, Test: {len(X_test)}")

# === 5. RECAT√âGORISATION DE TOUT LE DATASET ===
print_progress(5, "Pr√©diction sur l'ensemble du dataset")

# Pr√©diction avec gestion des cat√©gories rares
start_time = time.time()
batch_size = 10000  # Taille de lot pour la pr√©diction
total_rows = len(df)
predictions = []

print(f"üì¶ Traitement par lots de {batch_size} √©l√©ments...")

# Pr√©diction simple et efficace par lots
with parallel_backend('threading', n_jobs=-1):
    for i in range(0, total_rows, batch_size):
        batch_end = min(i + batch_size, total_rows)
        batch_data = df["clean_libelle"].iloc[i:batch_end]
        
        # Pr√©diction directe avec le pipeline
        batch_predictions = pipeline.predict(batch_data)
        predictions.extend(batch_predictions)
        
        # Affichage du progr√®s
        progress = (batch_end / total_rows) * 100
        print(f"   Lot {i//batch_size + 1}: {progress:.1f}% termin√© ({batch_end}/{total_rows})")

# Conversion des pr√©dictions avec gestion des cat√©gories filtr√©es
df["predicted_nature_encoded"] = predictions

# Cr√©ation d'une colonne pour les pr√©dictions avec gestion des cat√©gories rares
predicted_labels = []
for i, pred in enumerate(predictions):
    original_category = df.iloc[i][NATURE_COL]
    
    # Si la cat√©gorie originale n'√©tait pas dans l'entra√Ænement, la marquer comme rare
    if original_category not in valid_categories:
        predicted_labels.append("CAT√âGORIE_RARE")
    # Sinon, utiliser la pr√©diction du mod√®le
    elif pred < len(le_filtered.classes_):
        predicted_labels.append(le_filtered.inverse_transform([pred])[0])
    else:
        predicted_labels.append("CAT√âGORIE_RARE")  # S√©curit√© pour les pr√©dictions hors limites


PREDICTED_NATURE_COL = "predicted_nature_col"
df[PREDICTED_NATURE_COL] = predicted_labels
prediction_time = time.time() - start_time

# Calcul de la pr√©cision en excluant les cat√©gories rares des m√©triques
valid_mask = df[NATURE_COL].isin(valid_categories)
df_valid = df[valid_mask]

# Ajout d'une colonne VRAI/FAUX pour indiquer si la pr√©diction est correcte
df["prediction_correcte"] = df.apply(lambda row: 
    "VRAI" if row[NATURE_COL] == row[PREDICTED_NATURE_COL] else "FAUX", axis=1)

if len(df_valid) > 0:
    # Calcul de la pr√©cision SEULEMENT sur les cat√©gories que le mod√®le a apprises
    df_trainable = df[df[NATURE_COL].isin(valid_categories)]
    misclassified_trainable = (df_trainable[NATURE_COL] != df_trainable[PREDICTED_NATURE_COL]).sum()
    accuracy_trainable = ((len(df_trainable) - misclassified_trainable) / len(df_trainable)) * 100
    
    print(f"‚úÖ Pr√©dictions termin√©es en {prediction_time:.2f}s")
    print(f"üìä Pr√©cision (cat√©gories entra√Æn√©es): {accuracy_trainable:.1f}% ({misclassified_trainable}/{len(df_trainable)} erreurs)")
    
    # Statistiques globales (incluant les cat√©gories rares)
    correct_predictions = (df["prediction_correcte"] == "VRAI").sum()
    total_predictions = len(df)
    global_accuracy = (correct_predictions / total_predictions) * 100
    print(f"üìà Pr√©cision globale (toutes cat√©gories): {global_accuracy:.1f}% ({correct_predictions}/{total_predictions} correctes)")
    
    # Statistiques d√©taill√©es des cat√©gories rares
    rare_items = df[~df[NATURE_COL].isin(valid_categories)]
    rare_count = len(rare_items)
    if rare_count > 0:
        print(f"‚ö†Ô∏è  {rare_count} produits de cat√©gories rares marqu√©s comme 'CAT√âGORIE_RARE'")
        print(f"üìä R√©partition: {len(df_trainable)} entra√Ænables, {rare_count} rares")
else:
    print("‚úÖ Pr√©dictions termin√©es - Aucune cat√©gorie valide trouv√©e")

# === 6. EXTRACTION DES DIMENSIONS & COULEURS ===
print_progress(6, "Extraction des dimensions et couleurs")

# Mapping des variantes de couleurs vers les couleurs de base (source unique)
color_mapping = {
    # Couleurs de base
    "blanc": "blanc", "blanche": "blanc", "blancs": "blanc", "blanches": "blanc",
    "noir": "noir", "noire": "noir", "noirs": "noir", "noires": "noir",
    "gris": "gris", "grise": "gris", "grises": "gris",
    "rouge": "rouge", "rouges": "rouge",
    "bleu": "bleu", "bleue": "bleu", "bleus": "bleu", "bleues": "bleu",
    "vert": "vert", "verte": "vert", "verts": "vert", "vertes": "vert",
    "jaune": "jaune", "jaunes": "jaune",
    "rose": "rose", "roses": "rose",
    "violet": "violet", "violette": "violet", "violets": "violet", "violettes": "violet",
    "orange": "orange", "oranges": "orange",
    "turquoise": "turquoise", "turquoises": "turquoise",
    "cyan": "cyan", "cyans": "cyan",
    "magenta": "magenta", "magentas": "magenta",
    
    # Nuances de brun et bois
    "brun": "brun", "brune": "brun", "bruns": "brun", "brunes": "brun",
    "marron": "marron", "marrons": "marron",
    "bois": "bois", "bois√©": "bois", "bois√©e": "bois", "bois√©s": "bois", "bois√©es": "bois",
    "ch√™ne": "ch√™ne", "ch√™nes": "ch√™ne",
    "acajou": "acajou", "acajous": "acajou",
    "teck": "teck", "tecks": "teck",
    "bambou": "bambou", "bambous": "bambou",
    "pin": "pin", "pins": "pin",
    "√©rable": "√©rable", "√©rables": "√©rable",
    "noyer": "noyer", "noyers": "noyer",
    "h√™tre": "h√™tre", "h√™tres": "h√™tre",
    "fr√™ne": "fr√™ne", "fr√™nes": "fr√™ne",
    "merisier": "merisier", "merisiers": "merisier",
    "ch√¢taignier": "ch√¢taignier", "ch√¢taigniers": "ch√¢taignier",
    "cerisier": "cerisier", "cerisiers": "cerisier",
    
    # Couleurs pastel et nuances
    "beige": "beige", "beiges": "beige",
    "cr√®me": "cr√®me", "cr√®mes": "cr√®me",
    "ivoire": "ivoire", "ivoires": "ivoire",
    "√©cru": "√©cru", "√©crus": "√©cru",
    "taupe": "taupe", "taupes": "taupe",
    "sable": "sable", "sables": "sable",
    "ocre": "ocre", "ocres": "ocre",
    "terre": "terre", "terres": "terre",
    "camel": "camel", "camels": "camel",
    "caf√©": "caf√©", "caf√©s": "caf√©",
    "chocolat": "chocolat", "chocolats": "chocolat",
    "cognac": "cognac", "cognacs": "cognac",
    "caramel": "caramel", "caramels": "caramel",
    "miel": "miel", "miels": "miel",
    
    # Couleurs m√©talliques
    "argent": "argent", "argent√©": "argent", "argent√©e": "argent", "argent√©s": "argent", "argent√©es": "argent",
    "or": "or", "dor√©": "or", "dor√©e": "or", "dor√©s": "or", "dor√©es": "or",
    "bronze": "bronze", "bronz√©": "bronze", "bronz√©e": "bronze", "bronz√©s": "bronze", "bronz√©es": "bronze",
    "cuivre": "cuivre", "cuivr√©": "cuivre", "cuivr√©e": "cuivre", "cuivr√©s": "cuivre", "cuivr√©es": "cuivre",
    "laiton": "laiton", "laitons": "laiton",
    "chrome": "chrome", "chrom√©": "chrome", "chrom√©e": "chrome", "chrom√©s": "chrome", "chrom√©es": "chrome",
    "nickel": "nickel", "nickels": "nickel",
    "platine": "platine", "platines": "platine",
    
    # Couleurs sp√©ciales
    "transparent": "transparent", "transparente": "transparent", "transparents": "transparent", "transparentes": "transparent",
    "opaque": "opaque", "opaques": "opaque",
    "mat": "mat", "mate": "mat", "mats": "mat", "mates": "mat",
    "brillant": "brillant", "brillante": "brillant", "brillants": "brillant", "brillantes": "brillant",
    "satin√©": "satin√©", "satin√©e": "satin√©", "satin√©s": "satin√©", "satin√©es": "satin√©",
    "anthracite": "anthracite", "anthracites": "anthracite",
    "charbon": "charbon", "charbons": "charbon",
    "ardoise": "ardoise", "ardoises": "ardoise",
    "graphite": "graphite", "graphites": "graphite",
    
    # Couleurs nature
    "naturel": "naturel", "naturelle": "naturel", "naturels": "naturel", "naturelles": "naturel",
    "brut": "brut", "brute": "brut", "bruts": "brut", "brutes": "brut",
    "rustique": "rustique", "rustiques": "rustique",
    "vintage": "vintage", "vintages": "vintage",
    "antique": "antique", "antiques": "antique",
    "vieilli": "vieilli", "vieillie": "vieilli", "vieillis": "vieilli", "vieillies": "vieilli",
    
    # Couleurs vives
    "fluo": "fluo", "fluos": "fluo",
    "n√©on": "n√©on", "n√©ons": "n√©on",
    "phosphorescent": "phosphorescent", "phosphorescente": "phosphorescent", "phosphorescents": "phosphorescent", "phosphorescentes": "phosphorescent"
}

def extract_dimensions(text):
    text = str(text)
    
    # Patterns pour diff√©rents formats de dimensions
    patterns = [
        # Format 3D avec cm: "108 x 32 5 x 48 cm" -> "108 x 32.5 x 48"
        r'(\d+(?:[,\.]\d+)?)\s*[xX√ó*]\s*(\d+)\s+(\d+)\s*[xX√ó*]\s*(\d+)\s*cm',
        # Format 3D standard: "143 x 36 x 178 cm" ou "143 x 36 x 178"
        r'(\d+(?:[,\.]\d+)?)\s*[xX√ó*]\s*(\d+(?:[,\.]\d+)?)\s*[xX√ó*]\s*(\d+(?:[,\.]\d+)?)',
        # Format 2D: "45 x 75"
        r'(\d+(?:[,\.]\d+)?)\s*[xX√ó*]\s*(\d+(?:[,\.]\d+)?)'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            dimensions = []
            for match in matches:
                if len(match) == 4:  # Format sp√©cial "108 x 32 5 x 48"
                    # Combine le 2e et 3e groupe: "32" + "5" = "32.5"
                    dim1 = match[0].replace(',', '.')
                    dim2 = match[1] + '.' + match[2]  # "32" + "." + "5" = "32.5"
                    dim3 = match[3].replace(',', '.')
                    dimensions.append(f"{dim1}*{dim2}*{dim3}")
                elif len(match) == 3:  # Format 3D standard
                    dim_parts = [part.replace(',', '.') for part in match if part]
                    if len(dim_parts) == 3:
                        dimensions.append("*".join(dim_parts))
                elif len(match) == 2:  # Format 2D
                    dim_parts = [part.replace(',', '.') for part in match if part]
                    if len(dim_parts) == 2:
                        dimensions.append("*".join(dim_parts))
            
            if dimensions:
                return " | ".join(dimensions)
    
    return None

def extract_colors(text):
    text = str(text).lower()  # Convert to string first, then to lowercase
    
    found_colors = set()  # Utilise un set pour √©viter les doublons
    
    # Liste des adjectifs de couleur courants
    color_adjectives = [
        "clair", "claire", "clairs", "claires",
        "fonc√©", "fonc√©e", "fonc√©s", "fonc√©es", 
        "sombre", "sombres",
        "pale", "p√¢le", "pales", "p√¢les",
        "vif", "vive", "vifs", "vives",
        "intense", "intenses",
        "pastel", "pastels",
        "tendre", "tendres",
        "doux", "douce", "douces",
        "l√©ger", "l√©g√®re", "l√©gers", "l√©g√®res",
        "profond", "profonde", "profonds", "profondes"
    ]
    
    # Dictionnaire pour normaliser les adjectifs vers leur forme de base
    adjective_normalization = {
        "claire": "clair", "clairs": "clair", "claires": "clair",
        "fonc√©e": "fonc√©", "fonc√©s": "fonc√©", "fonc√©es": "fonc√©",
        "p√¢le": "p√¢le", "pales": "p√¢le", "p√¢les": "p√¢le",
        "vive": "vif", "vifs": "vif", "vives": "vif",
        "douce": "doux", "douces": "doux",
        "l√©g√®re": "l√©ger", "l√©gers": "l√©ger", "l√©g√®res": "l√©ger",
        "profonde": "profond", "profonds": "profond", "profondes": "profond"
    }
    
    # Pr√©paration des patterns
    all_color_variants = '|'.join(re.escape(couleur) for couleur in color_mapping.keys())
    adj_variants = '|'.join(re.escape(adj) for adj in color_adjectives)
    
    # 1. D'ABORD: chercher les couleurs avec adjectifs (priorit√© haute)
    combined_pattern = rf'\b(?:({all_color_variants})\s+({adj_variants})|({adj_variants})\s+({all_color_variants}))\b'
    
    # Collecter toutes les positions des matches avec adjectifs
    adjective_matches = []
    for match in re.finditer(combined_pattern, text):
        if match.group(1) and match.group(2):  # couleur + adjectif (ex: "gris clair")
            couleur_trouvee = match.group(1)
            adjectif_trouve = match.group(2)
            base_color = color_mapping[couleur_trouvee]
            # Normaliser l'adjectif avec le dictionnaire
            adj_normalized = adjective_normalization.get(adjectif_trouve, adjectif_trouve)
            
            color_with_adj = f"{base_color} {adj_normalized}"
            found_colors.add(color_with_adj)
            # Enregistrer la position pour √©viter les doublons
            adjective_matches.append((match.start(), match.end()))
            
        elif match.group(3) and match.group(4):  # adjectif + couleur (ex: "clair gris")
            couleur_trouvee = match.group(4)
            adjectif_trouve = match.group(3)
            base_color = color_mapping[couleur_trouvee]
            # Normaliser l'adjectif avec le dictionnaire
            adj_normalized = adjective_normalization.get(adjectif_trouve, adjectif_trouve)
            
            color_with_adj = f"{base_color} {adj_normalized}"
            found_colors.add(color_with_adj)
            # Enregistrer la position pour √©viter les doublons
            adjective_matches.append((match.start(), match.end()))
    
    # 2. ENSUITE: chercher les couleurs simples (en √©vitant les zones d√©j√† trouv√©es)
    simple_pattern = rf'\b({all_color_variants})\b'
    
    for match in re.finditer(simple_pattern, text):
        # V√©rifier si cette couleur n'est pas d√©j√† dans une combinaison avec adjectif
        is_in_adjective_match = False
        for adj_start, adj_end in adjective_matches:
            if adj_start <= match.start() < adj_end or adj_start < match.end() <= adj_end:
                is_in_adjective_match = True
                break
        
        # Ajouter seulement si ce n'est pas d√©j√† trouv√© avec un adjectif
        if not is_in_adjective_match:
            couleur_trouvee = match.group(1)
            base_color = color_mapping[couleur_trouvee]
            found_colors.add(base_color)
    
    return ", ".join(sorted(found_colors))

df["dimension_extraite"] = df[LIBELLE_COL].apply(extract_dimensions)
df["couleur_extraite"] = df[LIBELLE_COL].apply(extract_colors)

# Statistiques d'extraction
dimensions_found = df["dimension_extraite"].notna().sum()
colors_found = (df["couleur_extraite"] != "").sum()
print(f"‚úÖ Extraction termin√©e - Dimensions: {dimensions_found}, Couleurs: {colors_found}")

# === 7. EXPORT DES R√âSULTATS ===
print_progress(7, "Export des r√©sultats")

# G√©n√©ration d'un nom de fichier unique avec timestamp
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_filename = f"resultats_ecommerce_analyses_{timestamp}.xlsx"

# V√©rification que le fichier n'existe pas d√©j√† (s√©curit√© suppl√©mentaire)
counter = 1
base_filename = output_filename
while os.path.exists(output_filename):
    name_part = base_filename.replace('.xlsx', '')
    output_filename = f"{name_part}_{counter}.xlsx"
    counter += 1

df.to_excel(output_filename, index=False)

print(f"‚úÖ Script termin√©. Fichier '{output_filename}' g√©n√©r√©.")

# === 8. SAUVEGARDE DU MOD√àLE ===
print_progress(8, "Sauvegarde du mod√®le")

# Cr√©ation du dossier de sauvegarde
model_dir = "models"
os.makedirs(model_dir, exist_ok=True)

# G√©n√©ration des noms de fichiers avec timestamp
model_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

try:
    if USE_GPU and 'torch' in globals():
        # Sauvegarde mod√®le PyTorch GPU
        model_filename = f"{model_dir}/pytorch_model_{model_timestamp}.pth"
        tfidf_filename = f"{model_dir}/tfidf_configs_{model_timestamp}.pkl"
        label_encoder_filename = f"{model_dir}/label_encoder_{model_timestamp}.pkl"
        metadata_filename = f"{model_dir}/model_metadata_{model_timestamp}.pkl"
        
        # Sauvegarder le mod√®le PyTorch
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': {
                'input_size': input_size,
                'hidden_size': hidden_size,
                'num_classes': num_classes,
                'dropout_rate': 0.4
            },
            'device': DEVICE,
            'test_score': test_score
        }, model_filename, _use_new_zipfile_serialization=False)
        
        # Sauvegarder les configurations TF-IDF
        with open(tfidf_filename, 'wb') as f:
            pickle.dump(tfidf_configs, f)
        
        # Sauvegarder le label encoder
        with open(label_encoder_filename, 'wb') as f:
            pickle.dump(le_filtered, f)
        
        # Sauvegarder les m√©tadonn√©es
        metadata = {
            'model_type': 'pytorch_gpu',
            'timestamp': model_timestamp,
            'test_score': test_score,
            'num_classes': num_classes,
            'valid_categories': valid_categories.tolist(),
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'use_ensemble': False,
            'device': DEVICE
        }
        with open(metadata_filename, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"‚úÖ Mod√®le PyTorch sauvegard√©:")
        print(f"   üìÅ Mod√®le: {model_filename}")
        print(f"   üìÅ TF-IDF: {tfidf_filename}")
        print(f"   üìÅ Encodeur: {label_encoder_filename}")
        print(f"   üìÅ M√©tadonn√©es: {metadata_filename}")
        
    else:
        # Sauvegarde mod√®le CPU (ensemble ou standard)
        if USE_ENSEMBLE:
            model_filename = f"{model_dir}/ensemble_model_{model_timestamp}.pkl"
            model_type = 'ensemble_cpu'
        else:
            model_filename = f"{model_dir}/randomforest_model_{model_timestamp}.pkl"
            model_type = 'randomforest_cpu'
        
        label_encoder_filename = f"{model_dir}/label_encoder_{model_timestamp}.pkl"
        metadata_filename = f"{model_dir}/model_metadata_{model_timestamp}.pkl"
        
        # Sauvegarder le pipeline complet (TF-IDF + mod√®le)
        joblib.dump(pipeline, model_filename)
        
        # Sauvegarder le label encoder
        with open(label_encoder_filename, 'wb') as f:
            pickle.dump(le_filtered, f)
        
        # Sauvegarder les m√©tadonn√©es
        metadata = {
            'model_type': model_type,
            'timestamp': model_timestamp,
            'test_score': test_score,
            'num_classes': len(le_filtered.classes_),
            'valid_categories': valid_categories.tolist(),
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'use_ensemble': USE_ENSEMBLE,
            'device': 'cpu'
        }
        if not USE_ENSEMBLE:
            metadata['oob_score'] = pipeline.named_steps['clf'].oob_score_
        
        with open(metadata_filename, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"‚úÖ Mod√®le CPU sauvegard√©:")
        print(f"   üìÅ Pipeline: {model_filename}")
        print(f"   üìÅ Encodeur: {label_encoder_filename}")
        print(f"   üìÅ M√©tadonn√©es: {metadata_filename}")
    
    # Affichage du r√©sum√© de sauvegarde
    print(f"üìä R√©sum√© de la sauvegarde:")
    print(f"   üéØ Type: {metadata['model_type']}")
    print(f"   üìà Score test: {metadata['test_score']:.3f}")
    print(f"   üè∑Ô∏è  Classes: {metadata['num_classes']}")
    print(f"   üìö √âchantillons train/test: {metadata['training_samples']}/{metadata['test_samples']}")
    
except Exception as e:
    print(f"‚ùå Erreur lors de la sauvegarde du mod√®le: {e}")
    print("‚ö†Ô∏è  Le script continue malgr√© l'erreur de sauvegarde")

print("=" * 50)
print("üéâ Analyse e-commerce termin√©e avec succ√®s!")
print(f"üìÅ Mod√®les sauvegard√©s dans le dossier: {model_dir}/")
print("üí° Pour r√©utiliser le mod√®le, chargez les fichiers correspondants")
