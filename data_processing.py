"""
Module pour le chargement et le traitement des donn√©es.
Contient les fonctions de lecture de fichiers Excel et de pr√©paration des donn√©es.
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from text_processing import clean_text

def load_excel_data(file_path, libelle_col="Libell√© produit", nature_col="Nature", max_rows=None):
    """
    Charge les donn√©es depuis un fichier Excel avec option de limitation du nombre de lignes.
    
    Args:
        file_path (str): Chemin vers le fichier Excel
        libelle_col (str): Nom de la colonne contenant les libell√©s
        nature_col (str): Nom de la colonne contenant les natures
        max_rows (int, optional): Nombre maximum de lignes √† charger (None = toutes)
        
    Returns:
        pd.DataFrame: DataFrame avec les donn√©es charg√©es
    """
    print(f"üìÅ Chargement du fichier: {file_path}")
    
    if max_rows:
        print(f"‚ö†Ô∏è  Mode test: chargement limit√© √† {max_rows} lignes")
    
    # D√©tection automatique du format et utilisation de l'engine appropri√©
    if file_path.endswith('.xlsb'):
        try:
            # Essayer d'abord avec calamine (plus rapide pour .xlsb)
            if max_rows:
                df = pd.read_excel(file_path, engine='calamine', nrows=max_rows)
            else:
                df = pd.read_excel(file_path, engine='calamine')
        except:
            # Fallback vers pyxlsb
            if max_rows:
                df = pd.read_excel(file_path, engine='pyxlsb', nrows=max_rows)
            else:
                df = pd.read_excel(file_path, engine='pyxlsb')
    else:
        if max_rows:
            df = pd.read_excel(file_path, nrows=max_rows)
        else:
            df = pd.read_excel(file_path)
    
    # Filtrer pour ne garder que les colonnes n√©cessaires et supprimer les NaN
    df = df[[libelle_col, nature_col]].dropna()
    
    print(f"‚úÖ Donn√©es charg√©es: {len(df)} lignes, {len(df.columns)} colonnes")
    print(f"üìä Cat√©gories uniques: {len(df[nature_col].unique())}")
    
    if max_rows:
        print(f"üìã Chargement en mode test avec {len(df)} √©chantillons")
    else:
        print("üìã Chargement complet du fichier")
    
    return df

def prepare_data_for_training(df, libelle_col="LIBELLE", nature_col="NATURE", min_samples=30):
    """
    Pr√©pare les donn√©es pour l'entra√Ænement du mod√®le.
    
    Args:
        df (pd.DataFrame): DataFrame avec les donn√©es
        libelle_col (str): Nom de la colonne contenant les libell√©s
        nature_col (str): Nom de la colonne contenant les natures
        min_samples (int): Nombre minimum d'√©chantillons par cat√©gorie
        
    Returns:
        tuple: (df_filtered, valid_categories, category_counts)
    """
    print("üîç Analyse des cat√©gories...")
    
    # Calcul du nombre d'√©chantillons par cat√©gorie
    category_counts = df[nature_col].value_counts()
    
    # Filtrage des cat√©gories avec suffisamment d'√©chantillons
    valid_categories = category_counts[category_counts >= min_samples].index
    df_filtered = df[df[nature_col].isin(valid_categories)].copy()
    
    print(f"üìä Cat√©gories avec ‚â•{min_samples} √©chantillons: {len(valid_categories)}")
    print(f"üéØ √âchantillons utilis√©s: {len(df_filtered)} / {len(df)}")
    print(f"üìà R√©partition: min={category_counts[valid_categories].min()}, "
          f"max={category_counts[valid_categories].max()}, "
          f"moyenne={category_counts[valid_categories].mean():.1f}")
    
    return df_filtered, valid_categories, category_counts

def create_tfidf_vectorizers():
    """
    Cr√©e plusieurs configurations de vectoriseurs TF-IDF optimis√©es pour la m√©moire.
    
    Returns:
        list: Liste des configurations TF-IDF (comme dans l'original)
    """
    from numpy import float32
    
    tfidf_configs = [
        # Configuration 1: Features g√©n√©rales (r√©duit pour √©viter OOM)
        TfidfVectorizer(
            max_features=4000,  # R√©duit pour √©viter OOM
            ngram_range=(1, 2),  # R√©duit les n-grams
            min_df=3,
            max_df=0.9,
            dtype=float32,
            sublinear_tf=True,
            analyzer='word',
            token_pattern=r'\b[a-zA-Z]{2,}\b',
            stop_words=None,
            use_idf=True,
            smooth_idf=True,
            norm='l2',
            lowercase=False
        ),
        # Configuration 2: Focus sur les caract√®res (r√©duit pour √©viter OOM)
        TfidfVectorizer(
            max_features=2000,  # R√©duit pour √©viter OOM
            ngram_range=(2, 3),  # R√©duit les n-grams
            min_df=5,
            max_df=0.85,
            dtype=float32,
            sublinear_tf=True,
            analyzer='char',
            stop_words=None,
            use_idf=True,
            smooth_idf=True,
            norm='l2',
            lowercase=False
        )
    ]
    
    return tfidf_configs

def prepare_features(df, libelle_col, tfidf_configs):
    """
    Pr√©pare les features TF-IDF √† partir du texte de mani√®re efficace en m√©moire.
    
    Args:
        df (pd.DataFrame): DataFrame avec les donn√©es
        libelle_col (str): Nom de la colonne contenant les libell√©s
        tfidf_configs (list): Liste des configurations TF-IDF
        
    Returns:
        np.ndarray: Matrice de features combin√©es
    """
    print("üî§ Nettoyage du texte...")
    texts_cleaned = df[libelle_col].apply(clean_text)
    
    print("üî¢ Cr√©ation des features TF-IDF...")
    X_combined = []
    
    for i, vectorizer in enumerate(tfidf_configs):
        print(f"   üìä Configuration {i+1}: {vectorizer.analyzer} n-grams {vectorizer.ngram_range}")
        X_tfidf = vectorizer.fit_transform(texts_cleaned).toarray()
        X_combined.append(X_tfidf)
        print(f"      Dimensions: {X_tfidf.shape}")
    
    # Combinaison des features
    X_combined = np.hstack(X_combined)
    print(f"‚úÖ Features combin√©es: {X_combined.shape}")
    
    return X_combined

def prepare_labels(df, nature_col):
    """
    Pr√©pare les labels encod√©s pour l'entra√Ænement.
    
    Args:
        df (pd.DataFrame): DataFrame avec les donn√©es
        nature_col (str): Nom de la colonne contenant les natures
        
    Returns:
        tuple: (y_encoded, label_encoder)
    """
    print("üè∑Ô∏è  Encodage des labels...")
    
    le = LabelEncoder()
    y_encoded = le.fit_transform(df[nature_col])
    
    print(f"‚úÖ Labels encod√©s: {len(le.classes_)} classes uniques")
    
    return y_encoded, le
